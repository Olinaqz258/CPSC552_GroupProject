{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Applying VessGAN on COVID-19 CT\n",
        "\n",
        "Yiheng Zhou (yz996) | Eva Gao (eyg2) | Qiuyu Zhu (qz258) "
      ],
      "metadata": {
        "id": "X58d8i2QVFLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This notebook applies the VessGAN model on a novel data set*"
      ],
      "metadata": {
        "id": "Lc_nBItfVJiY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSJ8O6TTjLn8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "PsKDsBIGVL2O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C6onYrEieTM"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn \n",
        "import nibabel as nb\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import skimage.io \n",
        "import skimage.filters \n",
        "from scipy.ndimage import gaussian_filter \n",
        "from scipy import misc \n",
        "from torchvision import transforms, datasets \n",
        "from IPython.core.displayhook import Float\n",
        "from typing_extensions import dataclass_transform\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset \n",
        "from torch import optim as optim \n",
        "import math\n",
        "from torch.utils.data import DataLoader, Dataset \n",
        "from PIL import Image \n",
        "from google.colab.patches import cv2_imshow \n",
        "import torchvision.transforms as T "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing and define a CustomImage dataset "
      ],
      "metadata": {
        "id": "pEO7pWqqVRH4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlL-Jh4JjJVh"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "torch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvZ31NUbjU8N"
      },
      "outputs": [],
      "source": [
        "# define folder paths \n",
        "intermediate_data = \"/content/gdrive/MyDrive/Attention U-Net/Intermediate Data/\" \n",
        "\n",
        "# Load data train \n",
        "og_imgs = intermediate_data + \"train.nii\"\n",
        "binary_train_mask = \"/content/gdrive/MyDrive/Attention U-Net/Intermediate Data/binary_train_mask.nii\"\n",
        "binary_test_mask = \"/content/gdrive/MyDrive/Attention U-Net/Intermediate Data/binary_test_mask.nii\" "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define data loading/visualization helper functions and create dataloader "
      ],
      "metadata": {
        "id": "J3IE75l9Wd4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Data used is chest CT images and masks from the MedSeg COvID-19 dataset [2]* "
      ],
      "metadata": {
        "id": "ABtGBNkronct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_progress(real, fake, figsize=(10,5)): \n",
        "    '''\n",
        "    Displays the ground truth and generated masks. \n",
        "    '''\n",
        "    real = real.detach().cpu().permute(1, 2, 0)\n",
        "    fake = fake.detach().cpu().permute(1, 2, 0)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
        "    ax[0].imshow(real, cmap = 'gray')\n",
        "    ax[1].imshow(fake, cmap = 'gray')\n",
        "    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kVY-lmO4M9Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr-XA3l8jgbd"
      },
      "outputs": [],
      "source": [
        "# Data proccessing \n",
        "def load_nii(path):\n",
        "  img_file = nb.load(path)\n",
        "  imgs = img_file.get_fdata()\n",
        "  return imgs\n",
        "\n",
        "def convert_data(data,mask = False):\n",
        "  temp = np.array(data)\n",
        "  tensor = torch.tensor(temp).double() \n",
        "  tensor = torch.unsqueeze(tensor,0)\n",
        "  tensor = tensor.permute(3,0,1,2)\n",
        "  tensor = tensor[:30,:,:,:]\n",
        "  return tensor \n",
        "\n",
        "# img_path, mask_path = FULL path to nii file \n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, img_path, mask_path):\n",
        "        self.img = img_path\n",
        "        self.mask = mask_path \n",
        "\n",
        "    def __len__(self):\n",
        "        img = load_nii(self.img)\n",
        "        img_nii = convert_data(img)\n",
        "        return img_nii.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = load_nii(self.img)\n",
        "        mask = load_nii(self.mask) \n",
        "        img_nii = convert_data(img)\n",
        "        mask_nii = convert_data(mask,True)\n",
        "\n",
        "        img_slice = img_nii[idx, :, :, :] \n",
        "        mask_slice = mask_nii[idx, :, :, :]\n",
        "        \n",
        "        return img_slice, mask_slice \n",
        "\n",
        "# Data loader - batch size = 1 \n",
        "from torch.utils.data import DataLoader\n",
        "train_data = CustomImageDataset(og_imgs, binary_train_mask)\n",
        "dataloader = DataLoader(train_data, batch_size=1, shuffle=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve one sample from the dataset and visualize "
      ],
      "metadata": {
        "id": "8JQ566Z5WaVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img, mask = next(iter(dataloader))\n",
        "display_progress(img.squeeze(0), mask.squeeze(0)) "
      ],
      "metadata": {
        "id": "KLS-3gQhNUCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "8hgKlfY4WkB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The VessGAN is a modified variation of the vanilla GAN model, adapted from class and Goodfellow et al. 2014 [1].* "
      ],
      "metadata": {
        "id": "vhrv2Q5FWmYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator and Discriminator classes"
      ],
      "metadata": {
        "id": "7fFOr6cXWnav"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfmyRqAe_AmD"
      },
      "outputs": [],
      "source": [
        "# Create the Generator model class, which will be used to initialize the generator\n",
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    This class implements the generator. \n",
        "    It consists of four hidden layers each with a linear layer and a LeakyReLU activation function.\n",
        "    The last hidden layer uses a Tanh activation function.\n",
        "    '''\n",
        "  def __init__(self, input_dim, output_dim): \n",
        "    super(Generator,self).__init__() \n",
        "    self.hidden_layer1 = nn.Sequential(\n",
        "        nn.Linear(input_dim, 64),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "    self.hidden_layer2 = nn.Sequential(\n",
        "        nn.Linear(64, 128),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "    self.hidden_layer3 = nn.Sequential(\n",
        "        nn.Linear(128, 256),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "    self.hidden_layer4 = nn.Sequential(\n",
        "        nn.Linear(256, output_dim),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "    # ensure data type consistency by explicit casting \n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        m.weight.data = m.weight.data.type(torch.float32)\n",
        "        m.bias.data = m.bias.data.type(torch.float32)\n",
        "\n",
        "\n",
        "  def forward(self, x): \n",
        "      '''\n",
        "      Parameters: x: an input tensor of noise \n",
        "      Returns: output: a tensor of the generated image\n",
        "      '''\n",
        "      x = x.to(device) \n",
        "      x = x.float() \n",
        "      output = self.hidden_layer1(x)\n",
        "      output = self.hidden_layer2(output)\n",
        "      output = self.hidden_layer3(output)\n",
        "      output = self.hidden_layer4(output)\n",
        "      return output.to(device)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    '''\n",
        "    This class implements the discriminator.\n",
        "    It consists of four hidden layers each with a linear layer, \n",
        "    LeakyReLU activation function, and dropout. \n",
        "    The last hidden layer uses a Sigmoid activation function and no dropout. \n",
        "    '''\n",
        "    def __init__(self, input_dim, output_dim=1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.hidden_layer1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256), \n",
        "            nn.LeakyReLU(0.2), \n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.hidden_layer2 = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.hidden_layer3 = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.hidden_layer4 = nn.Sequential(\n",
        "            nn.Linear(64, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # ensure data type consistency by explicit casting \n",
        "        for m in self.modules():\n",
        "          if isinstance(m, nn.Linear):\n",
        "            m.weight.data = m.weight.data.type(torch.float32)\n",
        "            m.bias.data = m.bias.data.type(torch.float32)\n",
        "\n",
        "    def forward(self, x): \n",
        "        '''\n",
        "        Parameters: x: the generated image \n",
        "        Returns output: a tensor of logits\n",
        "        ''' \n",
        "        x = x.to(device)\n",
        "        x = x.float() \n",
        "        output = self.hidden_layer1(x)\n",
        "        output = self.hidden_layer2(output)\n",
        "        output = self.hidden_layer3(output)\n",
        "        output = self.hidden_layer4(output)\n",
        "        return output.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "TT1OsCxyW9oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define training procedures for generator and discriminator "
      ],
      "metadata": {
        "id": "-8SCRhjYXBuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug2SGQgD_gsZ"
      },
      "outputs": [],
      "source": [
        "# Training procedures \n",
        "lossf = nn.BCELoss()\n",
        "def train_generator(batch_size): \n",
        "    generator_optimizer.zero_grad()\n",
        "    noise = torch.randn(batch_size,100).to(device)\n",
        "    fake_img = generator(noise).to(device)\n",
        "    dis_out = discriminator(fake_img).to(device)\n",
        "    y = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "    loss = lossf(dis_out.float(), y.float())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    generator_optimizer.step()\n",
        "\n",
        "    return torch.sum(loss)/batch_size\n",
        "\n",
        "def train_discriminator(batch_size, images): \n",
        "    discriminator_optimizer.zero_grad()\n",
        "    noise = torch.randn(batch_size,100).to(device)\n",
        "    images = images.view(images.size(0), -1) \n",
        "    fake_img = generator(noise) \n",
        "  \n",
        "    true_y = torch.ones(batch_size, 1).to(device) \n",
        "    fake_y = torch.zeros(batch_size, 1).to(device) \n",
        "\n",
        "    fake_out = discriminator(fake_img)\n",
        "    true_out = discriminator(images)\n",
        "\n",
        "    loss_fake = lossf(fake_out.float(), fake_y.float())\n",
        "    loss_true = lossf(true_out.float(), true_y.float())\n",
        "\n",
        "    loss = (loss_fake + loss_true)/2\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    discriminator_optimizer.step()\n",
        "\n",
        "    return torch.sum(loss)/batch_size "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define training parameters"
      ],
      "metadata": {
        "id": "pCkqtGWUXsU5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY1VzrojAFbw"
      },
      "outputs": [],
      "source": [
        "training_parameters = {\n",
        "    \"img_size\": 512,\n",
        "    \"n_epochs\": 200,\n",
        "    \"batch_size\": 1,\n",
        "    \"learning_rate_generator\": 0.00002,\n",
        "    \"learning_rate_discriminator\": 0.00001,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize generator and discriminator models and their optimizers "
      ],
      "metadata": {
        "id": "-85A9yFGXx9r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAsDSKWq_Y61"
      },
      "outputs": [],
      "source": [
        "# initialize models  \n",
        "discriminator = Discriminator(262144,1).to(device) \n",
        "generator = Generator(100,262144).to(device)\n",
        "# initialize optimizers \n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=training_parameters['learning_rate_discriminator'])\n",
        "generator_optimizer = optim.Adam(generator.parameters(), lr=training_parameters['learning_rate_generator']) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop and output visualization "
      ],
      "metadata": {
        "id": "676RGx_5X0DT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpWtb65e_3F-"
      },
      "outputs": [],
      "source": [
        "d_loss = []\n",
        "g_loss = []\n",
        "for epoch in range(training_parameters['n_epochs']): \n",
        "    G_loss = []  \n",
        "    D_loss = []\n",
        "    for batch, (imgs, labels) in enumerate(dataloader):\n",
        "        batch_size = 1 \n",
        "        lossG = train_generator(batch_size) \n",
        "        G_loss.append(lossG)\n",
        "        lossD = train_discriminator(batch_size, labels) \n",
        "        lossD = train_discriminator(batch_size, labels) # second train \n",
        "        D_loss.append(lossD) \n",
        "        # Display a batch of generated images and print the loss \n",
        "        if(batch == 19): \n",
        "          noise = torch.randn(batch_size, 100).to(device) \n",
        "          fake = generator(noise).cpu().view(batch_size, 512, 512)\n",
        "          # display generated images and save losses \n",
        "          d_loss.append(torch.mean(torch.FloatTensor(D_loss)))\n",
        "          g_loss.append(torch.mean(torch.FloatTensor(G_loss)))\n",
        "          display_progress(labels[0], fake.unsqueeze(0)[0]) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loss"
      ],
      "metadata": {
        "id": "i4CRF-yXYEs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "g_loss = torch.tensor(g_loss) \n",
        "d_loss = torch.tensor(d_loss) \n",
        "\n",
        "g_loss = g_loss.detach().cpu().numpy() \n",
        "d_loss = d_loss.detach().cpu().numpy() \n",
        "\n",
        "df = pd.DataFrame(list(zip(g_loss, d_loss)), columns=[\"G_Loss\", \"D_Loss\"])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "auOggVkhYtoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns \n",
        "line_loss = sns.lineplot(df)"
      ],
      "metadata": {
        "id": "WiIi9J3wcHlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk4n8j9e1G6T"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Used to empty cache to avoid CUDA memory errors \n",
        "''' \n",
        "# import torch\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Citation"
      ],
      "metadata": {
        "id": "YZvNgL58Ux61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., & Bengio, Y. (2014). Generative adversarial networks. *Communications of the ACM* 63(11), 139-144., accessed at :\n",
        "https://dl.acm.org/doi/pdf/10.1145/3422622\n",
        "\n",
        "[2] MedSeg. (2020) COVID-19 CT segmentation dataset. *MedSeg: COVID-19*, accessed at: http://medicalsegmentation.com/covid19/ "
      ],
      "metadata": {
        "id": "wX_P-l2dUz0v"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}