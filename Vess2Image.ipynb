{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vess2Image \n",
        "\n",
        "Yiheng Zhou (yz996) | Eva Gao (eyg2) | Qiuyu Zhu (qz258) "
      ],
      "metadata": {
        "id": "7FD-Cq_-bWci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Translating vessel masks into retinal images*"
      ],
      "metadata": {
        "id": "RhMk7Cg2p7A3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKgveFh3LraK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "3w1ODHQtaUcS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE3ZxtA4f2Qh"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "from torch.utils.data import DataLoader, Dataset \n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGzi7m9_irEq"
      },
      "outputs": [],
      "source": [
        "original_images = '/content/drive/MyDrive/Deep Learning Group Project/DRIVE/original/images'\n",
        "original_masks = '/content/drive/MyDrive/Deep Learning Group Project/DRIVE/original/masks'\n",
        "original_eyeball = '/content/drive/MyDrive/Deep Learning Group Project/DRIVE/original/eyeballs'\n",
        "\n",
        "preprocessed_images = '/content/drive/MyDrive/Deep Learning Group Project/DRIVE/preprocessed/images'\n",
        "preprocessed_masks = '/content/drive/MyDrive/Deep Learning Group Project/DRIVE/preprocessed/masks'\n",
        "preprocessed_combined_masks = '/content/drive/MyDrive/Deep Learning Group Project/DRIVE/preprocessed/combined_masks'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load retinal images & binary vessel tree masks and save as PNG files "
      ],
      "metadata": {
        "id": "3rTxXOFRbhLp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw0-j6t7Loq-"
      },
      "outputs": [],
      "source": [
        "Nimgs = 20\n",
        "channels = 1\n",
        "height = 584\n",
        "width = 565\n",
        "\n",
        "imgs = np.empty((Nimgs,height,width,1))\n",
        "lbls = np.empty((Nimgs,height,width))\n",
        "eyeballs = np.empty((Nimgs,height,width))\n",
        "\n",
        "# data preprocessing\n",
        "i=0\n",
        "for file in sorted(os.listdir(original_images)):\n",
        "  img = cv2.imread(original_images+'/'+file, 1)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  img = np.moveaxis(img,0, -1)\n",
        "  imgs[i] = img\n",
        "  i += 1\n",
        "\n",
        "i=0\n",
        "for file in sorted(os.listdir(original_masks)):\n",
        "  g_truth = Image.open(original_masks +'/'+ file)\n",
        "  lbls[i] = np.asarray(g_truth)\n",
        "  i += 1\n",
        "\n",
        "i=0\n",
        "for file in sorted(os.listdir(original_eyeball)):\n",
        "  e_truth = Image.open(original_eyeball +'/'+ file)\n",
        "  eyeballs[i] = np.asarray(e_truth)\n",
        "  i += 1\n",
        "\n",
        "\n",
        "\n",
        "print(imgs.shape)\n",
        "print(lbls.shape)\n",
        "\n",
        "for i in range(20):\n",
        "  cv2.imwrite(preprocessed_images + '/' + str(i) + '.png', imgs[i])\n",
        "\n",
        "for i in range(20):\n",
        "  cv2.imwrite(preprocessed_masks + '/' + str(i) + '.png', lbls[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load eyeball background masks, combine with vessel tree structures, and save the combined masks as PNG files"
      ],
      "metadata": {
        "id": "8GeBvgP9bsPq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOACkIGiH5dP"
      },
      "outputs": [],
      "source": [
        "test = eyeballs.copy()\n",
        "\n",
        "preprocessed_combined_masks = '/content/drive/MyDrive/Deep Learning Group Project/DRIVE/preprocessed/combined_masks'\n",
        "\n",
        "for i in range(test.shape[0]):\n",
        "  for l in range(test.shape[1]):\n",
        "    for w in range(test.shape[2]):\n",
        "      if lbls[i,l,w] == 255:\n",
        "         test[i,l,w] = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuTxlf7TJLAW"
      },
      "outputs": [],
      "source": [
        "plt.imshow(test[0,:,:],cmap = 'gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jikvmYQMK1vC"
      },
      "outputs": [],
      "source": [
        "preprocessed_combined_masks = '/content/drive/MyDrive/Deep Learning Group Project/DRIVE/preprocessed/combined_masks'\n",
        "\n",
        " \n",
        "for i in range(20):\n",
        "    cv2.imwrite(preprocessed_combined_masks + '/' + str(i) + '.png', test[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader "
      ],
      "metadata": {
        "id": "1F_uUvaPdHxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Custom Dataset for retinal images and their corresponding masks (eyeball + vessel)"
      ],
      "metadata": {
        "id": "Vr-drgHZd0zC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Data used is retinal images and masks from the DRIVE: Grand Challenge [3]* "
      ],
      "metadata": {
        "id": "UC-WDGPyoLZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYFYtgpYBISR"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        self.imgs = sorted(os.listdir(path+'images'))\n",
        "        self.lbls = sorted(os.listdir(path+'combined_masks'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        transform = T.Resize(size = (512, 512))\n",
        "\n",
        "        img = self.imgs[idx]\n",
        "        img = Image.open(path+'images/'+ img)\n",
        "        img = transform(img)\n",
        "        img = np.asarray(img)\n",
        "        img = torch.tensor(img).unsqueeze(0).float()\n",
        "        \n",
        "        lbl = self.lbls[idx]\n",
        "        lbl = Image.open(path +'combined_masks/'+ lbl)\n",
        "        lbl = transform(lbl)\n",
        "        lbl = np.asarray(lbl)\n",
        "        lbl = torch.tensor(lbl).unsqueeze(0).float()\n",
        "        \n",
        "        return img, lbl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Anj4sDfjHYN8"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Deep Learning Group Project/DRIVE/preprocessed/'\n",
        "dataset = CustomImageDataset(path)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve one sample from the dataset and visualize "
      ],
      "metadata": {
        "id": "O7EERSRtdO6K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuADZeyqHjlj"
      },
      "outputs": [],
      "source": [
        "img, mask = next(iter(dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE55EhuXJMhH"
      },
      "outputs": [],
      "source": [
        "img.shape, mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN09f-PHeTxe"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(img.numpy()[0][0])\n",
        "cv2_imshow(mask.numpy()[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "aOS35V-ae72U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The Vess2Image is a modified variation of the Pix2Pix conditional GAN model [1], which uses a U-Net based architecture as a generator and a convolutional PatchGAN classifier as a discriminator.* "
      ],
      "metadata": {
        "id": "cViezHZkgGJy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttrN9ZqoLazh"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPslrsIlOI_T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import center_crop\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### U-Net based Generator"
      ],
      "metadata": {
        "id": "5cLQvay1gWjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The U-Net architecture consists of a contracting path (**DownSampleConv**) and an expanding path (**UpSampleConv**).*"
      ],
      "metadata": {
        "id": "SCkHMtLpgjMF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN0XatS3MOPT"
      },
      "outputs": [],
      "source": [
        "class DownSampleConv(nn.Module):\n",
        "    '''\n",
        "    This class implements the downsampling operations: 2DConvolution-BatchNorm-LeakyReLU\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel=4, stride=2, pad=1, batchnorm=True):\n",
        "        super().__init__()\n",
        "        self.batchnorm = batchnorm\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel, stride, pad)\n",
        "        if batchnorm:\n",
        "            self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.act = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Takes in a vector x and returns after downsampling\n",
        "        '''\n",
        "\n",
        "        x = self.conv(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.bn(x)\n",
        "        \n",
        "        x = self.act(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HxzSkv2MO0m"
      },
      "outputs": [],
      "source": [
        "class UpSampleConv(nn.Module):\n",
        "    '''\n",
        "    This class implements the upsampling operations: 2DConvolution-BatchNorm\n",
        "\n",
        "    '''\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel=4,\n",
        "        strides=2,\n",
        "        padding=1,\n",
        "        batchnorm=True,\n",
        "        dropout=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.batchnorm = batchnorm\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel, strides, padding)\n",
        "\n",
        "        if batchnorm:\n",
        "            self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        if dropout:\n",
        "            self.drop = nn.Dropout2d(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "    '''\n",
        "    Takes in a vector x and returns it after performing upsampling convolution\n",
        "    '''\n",
        "\n",
        "        x = self.deconv(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.bn(x)\n",
        "\n",
        "        if self.dropout:\n",
        "            x = self.drop(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWBcDGUWkLIY"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    This class implements the U-Net generator. \n",
        "    It consists of eight encoder blocks (downsampling) followed by seven decoder blocks (upsampling).\n",
        "    Skip connection is added between the encoder and the decoder states.\n",
        "    '''\n",
        "    def __init__(self, in_channels, out_channels): \n",
        "        super().__init__()\n",
        "        # encoders \n",
        "        self.enc1 = DownSampleConv(in_channels, 64, batchnorm=False)\n",
        "        self.enc2 = DownSampleConv(64, 128)\n",
        "        self.enc3 = DownSampleConv(128, 256)  \n",
        "        self.enc4 = DownSampleConv(256, 512)\n",
        "        self.enc5 = DownSampleConv(512, 512)\n",
        "        self.enc6 = DownSampleConv(512, 512)\n",
        "        self.enc7 = DownSampleConv(512, 512)\n",
        "        self.enc8 = DownSampleConv(512, 512, batchnorm = False)\n",
        "\n",
        "        # decoders\n",
        "        self.dec1 = UpSampleConv(512, 512, dropout=True) \n",
        "        self.dec2 = UpSampleConv(1024, 512, dropout=True) \n",
        "        self.dec3 = UpSampleConv(1024, 512, dropout=True) \n",
        "        self.dec4 = UpSampleConv(1024, 512) \n",
        "        self.dec5 = UpSampleConv(1024, 256) \n",
        "        self.dec6 = UpSampleConv(512, 128) \n",
        "        self.dec7 = UpSampleConv(256, 64) \n",
        "\n",
        "        self.final_conv = nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "        self.tanh = nn.Tanh() \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Parameters: x: an input tensor of image \n",
        "        Returns x: a tensor of the decoded image\n",
        "        '''\n",
        "\n",
        "        # encoders \n",
        "        skip1 = self.enc1(x)\n",
        "        skip2 = self.enc2(skip1) \n",
        "        skip3 = self.enc3(skip2) \n",
        "        skip4 = self.enc4(skip3) \n",
        "        skip5 = self.enc5(skip4) \n",
        "        skip6 = self.enc6(skip5) \n",
        "        skip7 = self.enc7(skip6) \n",
        "        enc_out = self.enc8(skip7) \n",
        "      \n",
        "        # decoders and add skip connections\n",
        "        d1 = self.dec1(enc_out)  \n",
        "        d1 = torch.cat((d1, skip7), axis=1) \n",
        "        d2 = self.dec2(d1)   \n",
        "        d2 = torch.cat((d2, skip6), axis=1) \n",
        "        d3 = self.dec3(d2)   \n",
        "        d3 = torch.cat((d3, skip5), axis=1) \n",
        "        d4 = self.dec4(d3)   \n",
        "        d4 = torch.cat((d4, skip4), axis=1) \n",
        "        d5 = self.dec5(d4)   \n",
        "        d5 = torch.cat((d5, skip3), axis=1) \n",
        "        d6 = self.dec6(d5) \n",
        "        d6 = torch.cat((d6, skip2), axis=1) \n",
        "\n",
        "        # last decoder \n",
        "        x = self.dec7(d6) \n",
        "        x = self.final_conv(x)\n",
        "        return self.tanh(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  PatchGAN Discriminator"
      ],
      "metadata": {
        "id": "Af7MEOUbiXia"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDixyAYrNr2y"
      },
      "outputs": [],
      "source": [
        "class PatchGAN(nn.Module):\n",
        "    '''\n",
        "    This class implements a convolutional PatchGAN classifier as a discriminator.\n",
        "    It consists of four successive downsampling blocks.\n",
        "    '''\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "        self.d1 = DownSampleConv(input_channels, 64, batchnorm=False)\n",
        "        self.d2 = DownSampleConv(64, 128)\n",
        "        self.d3 = DownSampleConv(128, 256)\n",
        "        self.d4 = DownSampleConv(256, 512)\n",
        "        self.final = nn.Conv2d(512, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        '''\n",
        "        Parameters: \n",
        "            x: the generated image\n",
        "            y: the conditioned image\n",
        "        Returns x5: a tensor of logits\n",
        "        '''\n",
        "        \n",
        "        x = torch.cat([x, y], axis=1)\n",
        "        x1 = self.d1(x)\n",
        "        x2 = self.d2(x1)\n",
        "        x3 = self.d3(x2)\n",
        "        x4 = self.d4(x3)\n",
        "        x5 = self.final(x4)\n",
        "        return x5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions: Weight initialization and Output Visualization "
      ],
      "metadata": {
        "id": "iHEPM9jnjBI5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFQSefpXmwvA"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    '''\n",
        "    Sets the weights of the two Conv2d operations,\n",
        "    ConvTranspose2d, and BatchNorm2d operation\n",
        "    '''\n",
        "  \n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias, 0)\n",
        "        \n",
        "def display_progress(cond, real, fake, figsize=(10,5)):\n",
        "    '''\n",
        "    Displays the ground truth masks, real images, and synthetic images.\n",
        "    '''\n",
        "    cond = cond.detach().cpu().permute(1, 2, 0)\n",
        "    real = real.detach().cpu().permute(1, 2, 0)\n",
        "    fake = fake.detach().cpu().permute(1, 2, 0)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
        "    ax[0].imshow(cond, cmap = 'gray')\n",
        "    ax[1].imshow(real, cmap = 'gray')\n",
        "    ax[2].imshow(fake, cmap = 'gray')\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vess2Image"
      ],
      "metadata": {
        "id": "aiKwwnnFjOND"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5ZzHO7bx0zS"
      },
      "outputs": [],
      "source": [
        "d_loss = []\n",
        "g_loss = [] \n",
        "class Pix2Pix(pl.LightningModule): \n",
        "    '''This class implements the Vess2Image framework\n",
        "    using PyTorch Lightning.'''\n",
        "    def __init__(self, in_channels, out_channels, learning_rate=0.00002, lambda_recon=160, display_step=5):\n",
        "        super().__init__()\n",
        "        # Define key variables\n",
        "        self.save_hyperparameters()\n",
        "        self.automatic_optimization = False # activate manual optimization\n",
        "        self.display_step = display_step\n",
        "        self.generator = Generator(in_channels, out_channels)\n",
        "        self.patchgan = PatchGAN(in_channels + out_channels)\n",
        "        self.generator = self.generator.apply(weights_init)\n",
        "        self.patchgan = self.patchgan.apply(weights_init) \n",
        "        self.adversarial_criterion = nn.BCEWithLogitsLoss()\n",
        "        self.recon_criterion = nn.L1Loss()\n",
        "\n",
        "    def _gen_step(self, real_images, conditioned_images):\n",
        "        # Define the generator training step\n",
        "        fake_images = self.generator(conditioned_images)\n",
        "        dis_logits = self.patchgan(fake_images, conditioned_images)\n",
        "        adversarial_loss = self.adversarial_criterion(dis_logits, torch.ones_like(dis_logits))\n",
        "        recon_loss = self.recon_criterion(fake_images, real_images)\n",
        "        lambda_recon = self.hparams.lambda_recon\n",
        "\n",
        "        return adversarial_loss + (lambda_recon * recon_loss)\n",
        "\n",
        "    def _dis_step(self, real_images, conditioned_images):\n",
        "        # Define the discriminator training step\n",
        "        fake_images = self.generator(conditioned_images).detach()\n",
        "        fake_logits = self.patchgan(fake_images, conditioned_images)\n",
        "\n",
        "        real_logits = self.patchgan(real_images, conditioned_images)\n",
        "\n",
        "        fake_loss = self.adversarial_criterion(fake_logits, torch.zeros_like(fake_logits))\n",
        "        real_loss = self.adversarial_criterion(real_logits, torch.ones_like(real_logits))\n",
        "        return (real_loss + fake_loss) / 2\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.hparams.learning_rate\n",
        "        gen_opt = torch.optim.Adam(self.generator.parameters(), lr=lr,betas = (0.5,0.5) )\n",
        "        dis_opt = torch.optim.Adam(self.patchgan.parameters(), lr=lr,betas  = (0.5,0.5))\n",
        "        return [dis_opt, gen_opt]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # Define training step for Pix2Pix\n",
        "        real, condition = batch\n",
        "        \n",
        "        # manual optimization\n",
        "        dis_opt, gen_opt = self.optimizers()\n",
        "        \n",
        "        # Train discriminator \n",
        "        dis_loss = self._dis_step(real, condition)\n",
        "\n",
        "        self.log('PatchGAN Loss', dis_loss)\n",
        "        dis_opt.zero_grad()\n",
        "        dis_loss.backward()\n",
        "        dis_opt.step()\n",
        "\n",
        "        # Train generator\n",
        "        gen_loss = self._gen_step(real, condition)\n",
        "\n",
        "        self.log('Generator Loss', gen_loss)\n",
        "        gen_opt.zero_grad()\n",
        "        gen_loss.backward()\n",
        "        gen_opt.step()\n",
        "        \n",
        "        # display ground-truth mask, real image, and synthetic image\n",
        "        if self.current_epoch%self.display_step==0 and batch_idx==0:\n",
        "            fake = self.generator(condition).detach()\n",
        "            display_progress(condition[0], real[0], fake[0])\n",
        "        \n",
        "        d_loss.append(dis_loss)\n",
        "        g_loss.append(gen_loss) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "qPbGOcpijfxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMsg8EsNOkbP"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Deep Learning Group Project/DRIVE/preprocessed/'\n",
        "dataset = CustomImageDataset(path)\n",
        "dataloader = DataLoader(dataset, batch_size=5, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRWnYe-sN0_I",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pix2pix = Pix2Pix(1, 1)\n",
        "trainer = pl.Trainer(max_epochs=30)\n",
        "trainer.fit(pix2pix, dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loss"
      ],
      "metadata": {
        "id": "crGRcxu7kTC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_d = []\n",
        "mean_g = []\n",
        "\n",
        "def get_mean_loss(losses, outlist, size):\n",
        "  temp = 0 \n",
        "  for i in range(len(losses)):\n",
        "    temp += losses[i]\n",
        "    if (i+1)%size == 0:\n",
        "      outlist.append(temp/size)\n",
        "  return torch.tensor(outlist)\n",
        "\n",
        "mean_d = get_mean_loss(d_loss, mean_d, 4)\n",
        "mean_g = get_mean_loss(g_loss, mean_g, 4)\n",
        "\n",
        "mean_d = mean_d.detach().cpu().numpy()\n",
        "mean_g = mean_g.detach().cpu().numpy()\n",
        "\n",
        "print(mean_d)\n",
        "print(mean_g)"
      ],
      "metadata": {
        "id": "Ljg8f5ltPNmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "df = pd.DataFrame(list(zip(mean_g, mean_d)), columns=[\"G_Loss\", \"D_Loss\"])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "tj8GisxzTr0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns \n",
        "line_loss = sns.lineplot(df)"
      ],
      "metadata": {
        "id": "-nIYH6SiUgoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Citation"
      ],
      "metadata": {
        "id": "LactJDhxsz9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Vess2Image model is adapted from parts of Mauryaâ€™s PyTorch Pix2Pix notebook [2]. Additionally, the model definitions for the Pix2Pix network are taken from the Image-to-image translation with conditional adversarial networks paper [1]. "
      ],
      "metadata": {
        "id": "EnZuXAqAy5_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] Isola, P., Zhu, J., Zhou ,T., & Efros, A.A. (2018). Image-to-image translation with conditional adversarial networks. arXiv, accessed at: https://arxiv.org/abs/1611.07004\n",
        "\n",
        "[2] Maurya, A. (2021). Pix2Pix-Image to image translation with conditional Adversarial Networks, accessed at: https://librecv.github.io/blog/gans/pytorch/2021/02/13/Pix2Pix-explained-with-code.html\n",
        "\n",
        "[3] Grand Challenge. (2023). DRIVE: digital retinal images for vessel extraction. *Grand Challenge*, accessed at: https://drive.grand-challenge.org/ "
      ],
      "metadata": {
        "id": "O8MX9Qt5rzqJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}