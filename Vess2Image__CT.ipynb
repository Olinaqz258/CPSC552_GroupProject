{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Vess2Image on COVID-19 CT\n",
        "\n",
        "Yiheng Zhou (yz996) | Eva Gao (eyg2) | Qiuyu Zhu (qz258) "
      ],
      "metadata": {
        "id": "2IM0v2oy1N4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This notebook applies the Vess2Image model on a novel data set*"
      ],
      "metadata": {
        "id": "SBehOctU1aD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn2Y7qcxo4hY",
        "outputId": "0b9d42ca-aa21-4269-bca0-d93ff7863580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "EXtpuWVK1t9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "dsMGhYdxp__1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI2SpAjbov08"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn \n",
        "import nibabel as nb\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n",
        "from scipy import misc \n",
        "from IPython.core.displayhook import Float\n",
        "from typing_extensions import dataclass_transform\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import center_crop\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm.auto import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "torch.set_default_dtype(torch.float64)"
      ],
      "metadata": {
        "id": "ajPr8vYmo1ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define folder paths  \n",
        "intermediate_data = \"/content/gdrive/MyDrive/Attention U-Net/Intermediate Data/\"\n",
        "\n",
        "# Load data train \n",
        "og_imgs = intermediate_data + \"train.nii\"\n",
        "binary_train_mask = \"/content/gdrive/MyDrive/Attention U-Net/Intermediate Data/binary_train_mask.nii\"\n",
        "binary_test_mask = \"/content/gdrive/MyDrive/Attention U-Net/Intermediate Data/binary_test_mask.nii\" "
      ],
      "metadata": {
        "id": "Zimx74QspD9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader"
      ],
      "metadata": {
        "id": "wplsEMSkrP9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define data loading/visualization helper functions and create dataloader "
      ],
      "metadata": {
        "id": "K08CS08j1-7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Data used is chest CT images and masks from the MedSeg COvID-19 dataset [3]* "
      ],
      "metadata": {
        "id": "jhv5rlzXn4XT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr-XA3l8jgbd"
      },
      "outputs": [],
      "source": [
        "# Data proccessing \n",
        "def load_nii(path):\n",
        "  img_file = nb.load(path)\n",
        "  imgs = img_file.get_fdata()\n",
        "  return imgs\n",
        "\n",
        "def convert_data(data,mask = False):\n",
        "  temp = np.array(data)\n",
        "  tensor = torch.tensor(temp).double() \n",
        "  tensor = torch.unsqueeze(tensor,0)\n",
        "  tensor = tensor.permute(3,0,1,2)\n",
        "  tensor = tensor[:30,:,:,:]\n",
        "  return tensor \n",
        "\n",
        "# img_path, mask_path = FULL path to nii file \n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, img_path, mask_path):\n",
        "        self.img = img_path\n",
        "        self.mask = mask_path \n",
        "\n",
        "    def __len__(self):\n",
        "        img = load_nii(self.img)\n",
        "        img_nii = convert_data(img)\n",
        "        return img_nii.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = load_nii(self.img)\n",
        "        mask = load_nii(self.mask) \n",
        "        img_nii = convert_data(img)\n",
        "        mask_nii = convert_data(mask,True)\n",
        "\n",
        "        img_slice = img_nii[idx, :, :, :] \n",
        "        mask_slice = mask_nii[idx, :, :, :]\n",
        "        \n",
        "        return img_slice, mask_slice \n",
        "\n",
        "# Data loader - batch size = 1 \n",
        "from torch.utils.data import DataLoader\n",
        "train_data = CustomImageDataset(og_imgs, binary_train_mask)\n",
        "train_dataloader = DataLoader(train_data, batch_size=1, shuffle=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture "
      ],
      "metadata": {
        "id": "cV11ZdtqpGoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The Vess2Image is a modified variation of the Pix2Pix conditional GAN model [1], which uses a U-Net based architecture as a generator and a convolutional PatchGAN classifier as a discriminator.* "
      ],
      "metadata": {
        "id": "RDBSUBjNpIt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### U-Net based generator"
      ],
      "metadata": {
        "id": "66ngoJyA2L_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The U-Net architecture consists of a contracting path (**DownSampleConv**) and an expanding path (**UpSampleConv**).*"
      ],
      "metadata": {
        "id": "_tK_X_IWrd25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSampleConv(nn.Module):\n",
        "    '''\n",
        "    This class implements the downsampling operations: 2DConvolution-BatchNorm-LeakyReLU\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel=4, stride=2, pad=1, batchnorm=True):\n",
        "        super().__init__()\n",
        "        self.batchnorm = batchnorm\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel, stride, pad)\n",
        "        if batchnorm:\n",
        "            self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.act = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Takes in a vector x and returns after downsampling\n",
        "        '''\n",
        "        x = self.conv(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.bn(x)\n",
        "        \n",
        "        x = self.act(x)\n",
        "        return x\n",
        "class UpSampleConv(nn.Module):\n",
        "    '''\n",
        "    This class implements the upsampling operations:2DConvolution-BatchNorm\n",
        "    '''\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel=4,\n",
        "        strides=2,\n",
        "        padding=1,\n",
        "        batchnorm=True,\n",
        "        dropout=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.batchnorm = batchnorm\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel, strides, padding)\n",
        "\n",
        "        if batchnorm:\n",
        "            self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "       \n",
        "        if dropout:\n",
        "            self.drop = nn.Dropout2d(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Takes in a vector x and returns it after performing upsampling convolution\n",
        "        '''\n",
        "        x = self.deconv(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.bn(x)\n",
        "\n",
        "        if self.dropout:\n",
        "            x = self.drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "l5iSXz8ZqlLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWBcDGUWkLIY"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    '''\n",
        "    This class implements the U-Net generator. \n",
        "    It consists of eight encoder blocks (downsampling) \n",
        "    followed by seven decoder blocks (upsampling).\n",
        "    Skip connection is added between the encoder and the decoder states.\n",
        "    '''\n",
        "    def __init__(self, in_channels, out_channels): \n",
        "        super().__init__()\n",
        "        # encoders \n",
        "        self.enc1 = DownSampleConv(in_channels, 64, batchnorm=False)\n",
        "        self.enc2 = DownSampleConv(64, 128)\n",
        "        self.enc3 = DownSampleConv(128, 256)  \n",
        "        self.enc4 = DownSampleConv(256, 512)\n",
        "        self.enc5 = DownSampleConv(512, 512)\n",
        "        self.enc6 = DownSampleConv(512, 512)\n",
        "        self.enc7 = DownSampleConv(512, 512)\n",
        "        self.enc8 = DownSampleConv(512, 512, batchnorm = False)\n",
        "\n",
        "        # decoders\n",
        "        self.dec1 = UpSampleConv(512, 512, dropout=True) \n",
        "        self.dec2 = UpSampleConv(1024, 512, dropout=True) \n",
        "        self.dec3 = UpSampleConv(1024, 512, dropout=True) \n",
        "        self.dec4 = UpSampleConv(1024, 512) \n",
        "        self.dec5 = UpSampleConv(1024, 256) \n",
        "        self.dec6 = UpSampleConv(512, 128) \n",
        "        self.dec7 = UpSampleConv(256, 64) \n",
        "\n",
        "        self.final_conv = nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "        self.tanh = nn.Tanh() \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Parameters: x an input tensor of image \n",
        "        Returns x: a tensor of the decoded image\n",
        "        '''\n",
        "        # encoders \n",
        "        skip1 = self.enc1(x)\n",
        "        skip2 = self.enc2(skip1) \n",
        "        skip3 = self.enc3(skip2) \n",
        "        skip4 = self.enc4(skip3) \n",
        "        skip5 = self.enc5(skip4) \n",
        "        skip6 = self.enc6(skip5) \n",
        "        skip7 = self.enc7(skip6) \n",
        "        enc_out = self.enc8(skip7) \n",
        "      \n",
        "        # decoders and add skip connections\n",
        "        d1 = self.dec1(enc_out)  \n",
        "        d1 = torch.cat((d1, skip7), axis=1) \n",
        "        d2 = self.dec2(d1)   \n",
        "        d2 = torch.cat((d2, skip6), axis=1) \n",
        "        d3 = self.dec3(d2)   \n",
        "        d3 = torch.cat((d3, skip5), axis=1) \n",
        "        d4 = self.dec4(d3)   \n",
        "        d4 = torch.cat((d4, skip4), axis=1) \n",
        "        d5 = self.dec5(d4)   \n",
        "        d5 = torch.cat((d5, skip3), axis=1) \n",
        "        d6 = self.dec6(d5) \n",
        "        d6 = torch.cat((d6, skip2), axis=1) \n",
        "\n",
        "        # last decoder \n",
        "        x = self.dec7(d6) \n",
        "        x = self.final_conv(x)\n",
        "        return self.tanh(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The PatchGAN discriminator"
      ],
      "metadata": {
        "id": "8cROJ3nY4MDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchGAN(nn.Module):\n",
        "    '''\n",
        "    This class implements a convolutional PatchGAN classifier as a discriminator.\n",
        "    It consists of four successive downsampling blocks.\n",
        "    '''\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "        self.d1 = DownSampleConv(input_channels, 64, batchnorm=False)\n",
        "        self.d2 = DownSampleConv(64, 128)\n",
        "        self.d3 = DownSampleConv(128, 256)\n",
        "        self.d4 = DownSampleConv(256, 512)\n",
        "        self.final = nn.Conv2d(512, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        '''\n",
        "        Parameters: \n",
        "            x: the generated image\n",
        "            y: the conditioned image\n",
        "        Returns x5: a tensor of logits\n",
        "        '''\n",
        "\n",
        "        x = torch.cat([x, y], axis=1)\n",
        "        x1 = self.d1(x)\n",
        "        x2 = self.d2(x1)\n",
        "        x3 = self.d3(x2)\n",
        "        x4 = self.d4(x3)\n",
        "        x5 = self.final(x4)\n",
        "        return x5"
      ],
      "metadata": {
        "id": "A2811EIJq9yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions: Weight initialization and Output Visualization "
      ],
      "metadata": {
        "id": "r0U5vl5d4_OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    '''\n",
        "    Sets the weights of the two Conv2d operations,\n",
        "    ConvTranspose2d, and BatchNorm2d operation\n",
        "    '''\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias, 0)\n",
        "        \n",
        "def display_progress(cond, real, fake, figsize=(10,5)):\n",
        "    '''\n",
        "    Displays the ground truth masks, real images, and synthetic images. \n",
        "    '''\n",
        "    cond = cond.detach().cpu().permute(1, 2, 0)\n",
        "    real = real.detach().cpu().permute(1, 2, 0)\n",
        "    fake = fake.detach().cpu().permute(1, 2, 0)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
        "    ax[0].imshow(cond, cmap = 'gray')\n",
        "    ax[1].imshow(real, cmap = 'gray')\n",
        "    ax[2].imshow(fake, cmap = 'gray')\n",
        "    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "855kufqDWtj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vess2Image"
      ],
      "metadata": {
        "id": "69UfjxCyrrQs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmltmSJ1Yza4"
      },
      "outputs": [],
      "source": [
        "d_loss = []\n",
        "g_loss = [] \n",
        "class Pix2Pix(pl.LightningModule): \n",
        "    '''This class implements the Vess2Image framework\n",
        "    using PyTorch Lightning.'''\n",
        "    def __init__(self, in_channels, out_channels, learning_rate=0.00002, lambda_recon=160, display_step=5):\n",
        "        # Define key variables\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.automatic_optimization = False\n",
        "        self.display_step = display_step\n",
        "        self.generator = Generator(in_channels, out_channels)\n",
        "        self.patchgan = PatchGAN(in_channels + out_channels)\n",
        "        self.generator = self.generator.apply(weights_init)\n",
        "        self.patchgan = self.patchgan.apply(weights_init) \n",
        "        self.adversarial_criterion = nn.BCEWithLogitsLoss()\n",
        "        self.recon_criterion = nn.L1Loss()\n",
        "\n",
        "    def _gen_step(self, real_images, conditioned_images):\n",
        "        # Define the generator training step\n",
        "        fake_images = self.generator(conditioned_images)\n",
        "        dis_logits = self.patchgan(fake_images, conditioned_images)\n",
        "        adversarial_loss = self.adversarial_criterion(dis_logits, torch.ones_like(dis_logits))\n",
        "        recon_loss = self.recon_criterion(fake_images, real_images)\n",
        "        lambda_recon = self.hparams.lambda_recon\n",
        "\n",
        "        return adversarial_loss + (lambda_recon * recon_loss)\n",
        "\n",
        "    def _dis_step(self, real_images, conditioned_images):\n",
        "        # Define the discriminator training step\n",
        "        fake_images = self.generator(conditioned_images).detach()\n",
        "        fake_logits = self.patchgan(fake_images, conditioned_images)\n",
        "\n",
        "        real_logits = self.patchgan(real_images, conditioned_images)\n",
        "\n",
        "        fake_loss = self.adversarial_criterion(fake_logits, torch.zeros_like(fake_logits))\n",
        "        real_loss = self.adversarial_criterion(real_logits, torch.ones_like(real_logits))\n",
        "        return (real_loss + fake_loss) / 2\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.hparams.learning_rate\n",
        "        gen_opt = torch.optim.Adam(self.generator.parameters(), lr=lr,betas = (0.5,0.5) )\n",
        "        dis_opt = torch.optim.Adam(self.patchgan.parameters(), lr=lr,betas  = (0.5,0.5))\n",
        "        return [dis_opt, gen_opt]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # Define training step for Pix2Pix\n",
        "        real, condition = batch\n",
        "\n",
        "        # manual optimization\n",
        "        dis_opt, gen_opt = self.optimizers()\n",
        "\n",
        "        # Train discriminator\n",
        "        dis_loss = self._dis_step(real, condition)\n",
        "\n",
        "        self.log('PatchGAN Loss', dis_loss)\n",
        "        dis_opt.zero_grad()\n",
        "        dis_loss.backward()\n",
        "        dis_opt.step()\n",
        "\n",
        "        # Train generator\n",
        "        gen_loss = self._gen_step(real, condition)\n",
        "\n",
        "        self.log('Generator Loss', gen_loss)\n",
        "        gen_opt.zero_grad()\n",
        "        gen_loss.backward()\n",
        "        gen_opt.step()\n",
        "        \n",
        "        # display ground-truth mask, real image, and synthetic image\n",
        "        if self.current_epoch%self.display_step==0 and batch_idx==0:\n",
        "            fake = self.generator(condition).detach()\n",
        "            display_progress(condition[0], real[0], fake[0])\n",
        "        \n",
        "        d_loss.append(dis_loss)\n",
        "        g_loss.append(gen_loss) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "_NgTyyPF44XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pix2pix = Pix2Pix(1, 1)\n",
        "trainer = pl.Trainer(max_epochs=30)\n",
        "trainer.fit(pix2pix, train_dataloader)"
      ],
      "metadata": {
        "id": "_j4KbXdKr3bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Citation"
      ],
      "metadata": {
        "id": "LfkqWYUInrRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] Isola, P., Zhu, J., Zhou ,T., & Efros, A.A. (2018). Image-to-image translation with conditional adversarial networks. arXiv, accessed at: https://arxiv.org/abs/1611.07004\n",
        "\n",
        "[2] Maurya, A. (2021). Pix2Pix-Image to image translation with conditional Adversarial Networks, accessed at: https://librecv.github.io/blog/gans/pytorch/2021/02/13/Pix2Pix-explained-with-code.html\n",
        "\n",
        "[3] MedSeg. (2020) COVID-19 CT segmentation dataset. *MedSeg: COVID-19*, accessed at: http://medicalsegmentation.com/covid19/ "
      ],
      "metadata": {
        "id": "oFNH43Ignsu0"
      }
    }
  ]
}